________________________________________
                                   Python PySpark ETL Pipeline â€“ Retail Sales Data

Project Overview
This project is a backend data processing system designed using production best practices for retail sales. It automates ingestion, validation, enrichment, transformation, and analytics-ready data mart creation using Python, PySpark, AWS S3, and MySQL.
The pipeline is modular, secure, and optimized, supporting multiple environments (dev/QA/prod), Spark partitioning, business calculations like salesperson incentives, and automated cleanup for production-grade workflows.
________________________________________
âš¡ Tech Stack
â€¢	Python 3.10 â€“ Core programming language
â€¢	PySpark â€“ Distributed data processing and transformations
â€¢	MySQL â€“ Dimension tables and data mart storage
â€¢	AWS S3 â€“ Raw and processed data storage
â€¢	Parquet â€“ Partitioned analytics-ready storage
â€¢	Faker â€“ Synthetic data generation for testing
â€¢	Docker â€“ Optional local Spark setup
â€¢	Logging & Utilities â€“ Encryption/decryption, AWS clients, Spark session, file management
________________________________________
âœ… Key Features
â€¢	Backend Data Processing: Retrieves raw files from AWS S3, validates schemas, and prepares structured outputs for analytics.
â€¢	Layered Architecture: Modular Python packages separate file handling, database access, business transformations, and utilities for maintainability.
â€¢	Secure AWS Integration: Encrypted credentials with custom modules; controlled read/write operations.
â€¢	Multiple Environments: Separate configs for dev, QA, and prod with environment-specific credentials, S3 buckets, and database connections.
â€¢	Schema Validation: Detects missing mandatory columns; separates invalid files for auditing.
â€¢	Data Enrichment: Joins raw sales data with MySQL dimension tables: Customers, Stores, Products, Sales Team.
â€¢	Customer Data Mart: Aggregates total purchases per customer.
â€¢	Sales Team Data Mart: Calculates monthly sales, salesperson rankings, and incentives; optimized with Spark window functions.
â€¢	Spark Optimization & Partitioning:
o	Writes partitioned Parquet files by sales_month and store_id for analytics performance
o	Uses window functions for ranking and aggregations
â€¢	Business Calculations: Incentive calculation for top-ranked salespersons (1% of total sales).
â€¢	Automated Cleanup & Staging Update: Moves processed files to S3, deletes local temporary files, and updates MySQL staging table status.
â€¢	Production-Ready Execution: Docker-based Spark setup, centralized logging, and environment-specific configs ensure consistent behavior across systems.
________________________________________
ğŸ“‚ Project Structure
Python-Pyspark-ETL/
â”œâ”€â”€ docs/ # Documentation and README
â”œâ”€â”€ resources/
â”‚ â”œâ”€â”€ dev/
â”‚ â”‚ â”œâ”€â”€ config.py # AWS, MySQL, S3 configs (dev)
â”‚ â”‚ â””â”€â”€ requirements.txt
â”‚ â”œâ”€â”€ qa/
â”‚ â”‚ â”œâ”€â”€ config.py # AWS, MySQL, S3 configs (qa)
â”‚ â”‚ â””â”€â”€ requirements.txt
â”‚ â”œâ”€â”€ prod/
â”‚ â”‚ â”œâ”€â”€ config.py # AWS, MySQL, S3 configs (prod)
â”‚ â”‚ â””â”€â”€ requirements.txt
â”‚ â””â”€â”€ sql_scripts/
â”‚ â””â”€â”€ table_scripts.sql # Dimension & staging table creation
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ main/
â”‚ â”‚ â”œâ”€â”€ delete/
â”‚ â”‚ â”‚ â”œâ”€â”€ aws_delete.py
â”‚ â”‚ â”‚ â”œâ”€â”€ database_delete.py
â”‚ â”‚ â”‚ â””â”€â”€ local_file_delete.py
â”‚ â”‚ â”œâ”€â”€ download/
â”‚ â”‚ â”‚ â””â”€â”€ aws_file_download.py
â”‚ â”‚ â”œâ”€â”€ move/
â”‚ â”‚ â”‚ â””â”€â”€ move_files.py
â”‚ â”‚ â”œâ”€â”€ read/
â”‚ â”‚ â”‚ â”œâ”€â”€ aws_read.py
â”‚ â”‚ â”‚ â””â”€â”€ database_read.py
â”‚ â”‚ â”œâ”€â”€ transformations/jobs/
â”‚ â”‚ â”‚ â”œâ”€â”€ customer_mart_sql_transform_write.py
â”‚ â”‚ â”‚ â”œâ”€â”€ dimension_tables_join.py
â”‚ â”‚ â”‚ â”œâ”€â”€ main.py
â”‚ â”‚ â”‚ â””â”€â”€ sales_mart_sql_transform_write.py
â”‚ â”‚ â”œâ”€â”€ upload/
â”‚ â”‚ â”‚ â””â”€â”€ upload_to_s3.py
â”‚ â”‚ â”œâ”€â”€ utility/
â”‚ â”‚ â”‚ â”œâ”€â”€ encrypt_decrypt.py
â”‚ â”‚ â”‚ â”œâ”€â”€ logging_config.py
â”‚ â”‚ â”‚ â”œâ”€â”€ s3_client_object.py
â”‚ â”‚ â”‚ â”œâ”€â”€ spark_session.py
â”‚ â”‚ â”‚ â””â”€â”€ mysql_connection.py
â”‚ â”‚ â””â”€â”€ write/
â”‚ â”‚ â”œâ”€â”€ database_write.py
â”‚ â”‚ â””â”€â”€ parquet_write.py
â”‚ â””â”€â”€ test/
â”‚ â”œâ”€â”€ scratch_pad.py
â”‚ â””â”€â”€ generate_csv_data.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
________________________________________
âš™ï¸ Step-by-Step Implementation
1.	Synthetic Data Generation
o	Customers, stores, products, salespersons, and transactions generated with Faker.
2.	AWS S3 Integration
o	Secure download of raw CSVs, upload of processed Parquet files.
3.	Schema Validation
o	Checks mandatory columns and separates invalid files.
4.	Data Enrichment
o	Joins raw data with MySQL dimension tables for analytics-ready facts.
5.	Data Mart Creation
o	Customer Data Mart: Total purchases per customer
o	Sales Team Data Mart: Monthly sales, rankings, incentives
6.	Partitioning & Spark Optimization
o	Writes partitioned Parquet by sales_month and store_id
o	Uses window functions for ranking and aggregations
7.	Business Calculations
o	Incentive for top-ranked salesperson (1% of total sales)
8.	Cleanup & Staging Table Update
o	Moves processed files to S3
o	Deletes local temporary files
o	Updates MySQL staging table status
9.	Production Readiness
o	Docker-based Spark setup for local testing
o	Centralized logging for audit and debugging
o	Environment-specific configs ensure consistency across dev, QA, and production
ğŸ“ˆ Performance Observations (Local Execution)
â€¢	Tested with ~500,000 synthetic retail transactions.
â€¢	Converting CSV to Parquet reduced storage by 55â€“65%.
â€¢	Reading Parquet improved query performance by 35â€“45%.
â€¢	Partitioning by sales_month and store_id reduced scan time by 30â€“40% for monthly analytics.
â€¢	End-to-end ETL execution completed in 1â€“2 minutes on local dev environment (8â€“16 GB RAM).
________________________________________
ğŸ“Š Final Deliverables
â€¢	Automated ETL pipeline: S3 â†’ PySpark â†’ MySQL â†’ Parquet â†’ S3
â€¢	Customer and Sales Team Data Marts with KPI calculations
â€¢	Partitioned and optimized Parquet storage
â€¢	Secure handling of AWS credentials
â€¢	Modular, layered architecture for maintainability and scalability
â€¢	Production-ready workflow with Docker, logging, and environment separation
â€¢	Synthetic dataset for testing and demos
________________________________________
Author: Utkarsh Reddy Nathala
LinkedIn: https://www.linkedin.com/in/utkarsh-reddy-nathala
________________________________________

