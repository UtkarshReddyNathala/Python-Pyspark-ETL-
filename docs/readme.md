# **Python PySpark ETL Pipeline – Retail Sales Data**

This project is a backend data processing system designed using production best practices for retail sales. It automates ingestion, validation, enrichment, transformation, and analytics-ready data mart creation using Python, PySpark, AWS S3, and MySQL.

The pipeline is modular, secure, and optimized, supporting multiple environments (dev/QA/prod), Spark partitioning, business calculations like salesperson incentives, and automated cleanup for production-grade workflows.

---
##  Tech Stack

* **Python 3.10** – Core programming language
* **PySpark** – Distributed data processing
* **MySQL** – Dimension tables & data marts
* **AWS S3** – Raw & processed data storage
* **Parquet** – Partitioned, analytics-ready storage
* **Faker** – Synthetic data generation
* **Docker** – Optional local Spark setup
* **Logging & Utilities** – Encryption,decryption, AWS clients, Spark session management, file handling

---

<h2 align="center">Data Architecture</h2>
<p align="center">
  <img src="architecture.png" width="600">
</p>

<h2 align="center">Data Model</h2>
<p align="center">
  <img src="database_schema.drawio.png" width="600">
</p> 

---

## Project Structure
A modular **AWS + Spark + MySQL + S3 based data processing pipeline** supporting multiple environments (**dev, qa, prod**) with transformation jobs for staging, dimension, and mart tables.

```
my_project/
│
├── docs/                                  # Documentation and README
│   └── readme.md
│
├── resources/
│   ├── __init__.py
│   │
│   ├── dev/                               # AWS, MySQL, S3 configs (dev)
│   │   ├── config.py
│   │   └── requirement.txt
│   │
│   ├── qa/                                # AWS, MySQL, S3 configs (qa)
│   │   ├── config.py
│   │   └── requirement.txt
│   │
│   ├── prod/                              # AWS, MySQL, S3 configs (prod)
│   │   ├── config.py
│   │   └── requirement.txt
│   │
│   └── sql_scripts/                       # Dimension & staging table creation
│       └── table_scripts.sql
│
├── src/
│   ├── main/
│   │   ├── __init__.py
│   │
│   │   ├── delete/                        # Delete operations
│   │   │   ├── aws_delete.py
│   │   │   ├── database_delete.py
│   │   │   └── local_file_delete.py
│   │   │
│   │   ├── download/                      # Download from S3
│   │   │   └── aws_file_download.py
│   │   │
│   │   ├── move/                          # File movement logic
│   │   │   └── move_files.py
│   │   │
│   │   ├── read/                          # Read from AWS / Database
│   │   │   ├── aws_read.py
│   │   │   └── database_read.py
│   │   │
│   │   ├── transformations/               # Data transformation layer
│   │   │   └── jobs/
│   │   │       ├── customer_mart_sql_transform_write.py
│   │   │       ├── sales_mart_sql_transform_write.py
│   │   │       ├── dimension_tables_join.py
│   │   │       └── main.py
│   │   │
│   │   ├── upload/                        # Upload to S3
│   │   │   └── upload_to_s3.py
│   │   │
│   │   ├── utility/                       # Common utilities
│   │   │   ├── encrypt_decrypt.py
│   │   │   ├── logging_config.py
│   │   │   ├── s3_client_object.py
│   │   │   ├── spark_session.py
│   │   │   └── my_sql_session.py
│   │   │
│   │   └── write/                         # Write to DB / Parquet
│   │       ├── database_write.py
│   │       └── parquet_write.py
│   │
│   └── test/
│       ├── scratch_pad.py
│       └── generate_csv_data.py
```

---

## Data Flow and Features

* **Backend Data Processing**: Retrieves raw files from AWS S3 and prepares structured outputs for analytics.
* **Schema Validation**:

  * Checks mandatory columns.
  * Separates invalid files for auditing.
* **Data Enrichment**: Joins validated data with MySQL dimension tables — Customers, Stores, Products, and Sales Team.
* **Data Marts**:

  * **Customer Data Mart**: Aggregates total purchases per customer.
  * **Sales Team Data Mart**: Calculates monthly sales and rankings using Spark **window functions**
* **Partitioning & Spark Optimization**:

  * Writes partitioned Parquet files by `sales_month` and `store_id` for analytics performance.
  * Uses Spark window functions for ranking and aggregations.
* **Business Calculations**:

  * Calculates incentives for top-ranked salespersons.
  * Top salesperson receives 1% of total sales.
* **Automated Cleanup & Staging Update**:

  * Moves processed files to S3.
  * Deletes local temporary files.
  * Updates MySQL staging table status.
* **Production-Ready Execution**:

  * Docker-based Spark setup for local or distributed execution.
  * Centralized logging for audit and debugging.
  * Environment-specific configs for dev, QA, and prod.
* **Layered Architecture**: Modular Python packages manage file operations, database access, transformations, and utilities for maintainability and scalability.
* **Data Generation**: Generates synthetic Customers, Stores, Products, Salespersons, and Transactions using Faker for testing/demo purposes.
* **AWS S3 Integration**:

  * Securely downloads raw CSVs.
  * Uploads processed Parquet files to S3.
    
---

## Performance Observations (Local Execution)

* Tested with **~500,000** synthetic retail transactions.
* Converting CSV to Parquet reduced storage by **~59%** (from ~1.9 GB → ~780 MB).
* Reading Parquet improved query performance by **~43%** (total sales/month: 14s → 8s).
* Partitioning by **sales_month** and **store_id** reduced scan time by **~31%** for monthly analytics (11.5s → 8s).
* End-to-end ETL execution completed in **~1–2 minutes** on local dev environment (8–16 GB RAM).

---
## Final Deliverables

* Automated ETL pipeline: S3 → PySpark → MySQL → Parquet → S3
* Customer & Sales Team Data Marts with KPIs
* Partitioned, optimized Parquet storage
* Secure AWS credential handling
* Modular, layered architecture for maintainability
* Production-ready workflow: Docker, logging, environment separation
* Synthetic dataset for testing/demo
  
---

 **How to Run the Project**

1. **Clone the repository**

   ```bash
   git clone <your_repo_url>
   cd Python-Pyspark-ETL
   ```

2. **Set up environment**

   * Install Python dependencies:

   ```bash
   pip install -r requirements.txt
   ```

   * Configure AWS, MySQL, and environment settings in `resources/dev/config.py` (or QA/prod)

3. **Run locally with Docker Spark (optional)**

   * Start Spark session using Docker if needed for distributed execution

4. **Execute ETL pipeline**

   ```bash
   python src/main/transformations/jobs/main.py
   ```

5. **Check processed files**

   * Verify S3 for partitioned Parquet files and MySQL for updated data marts

---

**Author:** Utkarsh Reddy Nathala

**LinkedIn:** https://www.linkedin.com/in/utkarsh-reddy-nathala-b5b56728a/

